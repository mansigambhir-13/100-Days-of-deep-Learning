{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlQowckYHKwrhWzvrgQgs1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mansigambhir-13/100-Days-of-deep-Learning/blob/main/Text_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vlv2vBuDp8i9",
        "outputId": "6265fe7b-da5c-4cd7-dffa-3acdd1d38d35"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              review  \\\n",
            "0  One of the other reviewers has mentioned that ...   \n",
            "1  A wonderful little production. <br /><br />The...   \n",
            "2  I thought this was a wonderful way to spend ti...   \n",
            "3  Basically there's a family where a little boy ...   \n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...   \n",
            "\n",
            "                                      cleaned_review  \\\n",
            "0  one of the other reviewers has mentioned that ...   \n",
            "1  a wonderful little production the filming tech...   \n",
            "2  i thought this was a wonderful way to spend ti...   \n",
            "3  basically theres a family where a little boy j...   \n",
            "4  petter matteis love in the time of money is a ...   \n",
            "\n",
            "                                        no_stopwords  \\\n",
            "0  one reviewers mentioned watching oz episode yo...   \n",
            "1  wonderful little production filming technique ...   \n",
            "2  thought wonderful way spend time hot summer we...   \n",
            "3  basically theres family little boy jake thinks...   \n",
            "4  petter matteis love time money visually stunni...   \n",
            "\n",
            "                                             stemmed  \n",
            "0  one review mention watch oz episod youll hook ...  \n",
            "1  wonder littl product film techniqu unassum old...  \n",
            "2  thought wonder way spend time hot summer weeke...  \n",
            "3  basic there famili littl boy jake think there ...  \n",
            "4  petter mattei love time money visual stun film...  \n",
            "Preprocessing complete. Preprocessed data saved to 'preprocessed_imdb_reviews.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the IMDB dataset\n",
        "# The dataset is in a CSV file named 'imdb_reviews.csv' with columns 'review' and 'sentiment'\n",
        "df = pd.read_csv('/content/IMDB Dataset.csv')\n",
        "\n",
        "# Function to clean the text\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Remove punctuation and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "# Function to remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def remove_stopwords(text):\n",
        "    words = word_tokenize(text)\n",
        "    return ' '.join([word for word in words if word not in stop_words])\n",
        "\n",
        "# Initialize Porter Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Function to perform stemming\n",
        "def stem_words(text):\n",
        "    words = word_tokenize(text)\n",
        "    return ' '.join([stemmer.stem(word) for word in words])\n",
        "\n",
        "# Apply preprocessing steps\n",
        "df['cleaned_review'] = df['review'].apply(clean_text)\n",
        "df['no_stopwords'] = df['cleaned_review'].apply(remove_stopwords)\n",
        "df['stemmed'] = df['no_stopwords'].apply(stem_words)\n",
        "\n",
        "# Display the first few rows to check the results\n",
        "print(df[['review', 'cleaned_review', 'no_stopwords', 'stemmed']].head())\n",
        "\n",
        "# Save the preprocessed data\n",
        "df.to_csv('preprocessed_imdb_reviews.csv', index=False)\n",
        "\n",
        "print(\"Preprocessing complete. Preprocessed data saved to 'preprocessed_imdb_reviews.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#removing the chat words from the dataset"
      ],
      "metadata": {
        "id": "a5h9VMnxvil-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_words = {\n",
        "    'AFAIK': 'as far as I know',\n",
        "    'AMA': 'ask me anything',\n",
        "    'ASAP': 'as soon as possible',\n",
        "    'BRB': 'be right back',\n",
        "    'BTW': 'by the way',\n",
        "    'FYI': 'for your information',\n",
        "    'IIRC': 'if I remember correctly',\n",
        "    'IMO': 'in my opinion',\n",
        "    'IMHO': 'in my humble opinion',\n",
        "    'LOL': 'laugh out loud',\n",
        "    'ROFL': 'rolling on the floor laughing',\n",
        "    'TBH': 'to be honest',\n",
        "    'TLDR': 'too long didn\\'t read',\n",
        "    'TTYL': 'talk to you later',\n",
        "    'WTF': 'what the fuck',\n",
        "    'YOLO': 'you only live once'\n",
        "}"
      ],
      "metadata": {
        "id": "8W1NT0ChwNq5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_chat_words(text):\n",
        "    words = text.split()\n",
        "    return ' '.join([chat_words.get(word.upper(), word) for word in words])"
      ],
      "metadata": {
        "id": "_3jxSmCSwMxl"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This approach will replace common chat acronyms with their full-text equivalents, making the text more standardized and easier to process. You can expand the chat_words dictionary with more acronyms as needed for your specific dataset."
      ],
      "metadata": {
        "id": "_f6lBm9EwMau"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Spelling checking and correction to the dataset"
      ],
      "metadata": {
        "id": "M_n9DoOuwwjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspellchecker"
      ],
      "metadata": {
        "id": "EBc2tgshxd8U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0bce631-ded3-4b16-d996-7cc795421b78"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.10/dist-packages (0.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spellchecker import SpellChecker"
      ],
      "metadata": {
        "id": "UYgF2SjZw7B-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize SpellChecker\n",
        "spell = SpellChecker()"
      ],
      "metadata": {
        "id": "-Y5-YHfRwbkX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_spelling(text):\n",
        "    words = text.split()\n",
        "    corrected_words = [spell.correction(word) or word for word in words]\n",
        "    return ' '.join(corrected_words)"
      ],
      "metadata": {
        "id": "nnuf6qYOxG5d"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenization of the text"
      ],
      "metadata": {
        "id": "7Y2-OV-KB25I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word_tokenize_text(self, text):\n",
        "        \"\"\"Perform word-level tokenization\"\"\"\n",
        "        tokens = self.word_tokenizer.tokenize(text)\n",
        "        tokens = [token for token in tokens if token not in self.stop_words]\n",
        "        return tokens"
      ],
      "metadata": {
        "id": "_FwgnzsaxI1B"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def subword_tokenize_text(self, text):\n",
        "        \"\"\"Perform subword tokenization using BERT tokenizer\"\"\"\n",
        "        tokens = self.bert_tokenizer.tokenize(text)\n",
        "        tokens = [token for token in tokens if token not in self.stop_words]\n",
        "        return tokens"
      ],
      "metadata": {
        "id": "5rSDBOwdClk9"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_tokenize_text(self, text):\n",
        "        \"\"\"Perform sentence-level tokenization\"\"\"\n",
        "        return sent_tokenize(text)\n",
        "def process_chunk(self, texts):\n",
        "        \"\"\"Process a chunk of texts\"\"\"\n",
        "        results = []\n",
        "        for text in texts:\n",
        "            preprocessed_text = self.preprocess_text(text)\n",
        "\n",
        "            if self.tokenization_method == 'word':\n",
        "                tokens = self.word_tokenize_text(preprocessed_text)\n",
        "            elif self.tokenization_method == 'subword':\n",
        "                tokens = self.subword_tokenize_text(preprocessed_text)\n",
        "            elif self.tokenization_method == 'sentence':\n",
        "                tokens = self.sentence_tokenize_text(preprocessed_text)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown tokenization method: {self.tokenization_method}\")\n",
        "\n",
        "            results.append(tokens)\n",
        "        return results"
      ],
      "metadata": {
        "id": "zVuCUld5CpVi"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_dataset(self, texts):\n",
        "        \"\"\"Tokenize the entire dataset using multiprocessing\"\"\"\n",
        "        # Split texts into chunks for parallel processing\n",
        "        chunk_size = len(texts) // self.num_processes\n",
        "        chunks = [texts[i:i + chunk_size] for i in range(0, len(texts), chunk_size)]\n",
        "\n",
        "        # Process chunks in parallel\n",
        "        with mp.Pool(self.num_processes) as pool:\n",
        "            results = list(tqdm(pool.imap(self.process_chunk, chunks),\n",
        "                              total=len(chunks),\n",
        "                              desc=\"Tokenizing texts\"))\n",
        "\n",
        "        # Flatten results\n",
        "        return [token for chunk_result in results for token in chunk_result]\n"
      ],
      "metadata": {
        "id": "FHCHX2uRCxhH"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class IMDBTokenizer:\n",
        "    def __init__(self, tokenization_method='word', num_processes=None):\n",
        "        self.tokenization_method = tokenization_method\n",
        "        self.num_processes = num_processes or mp.cpu_count()\n",
        "\n",
        "        # Download NLTK resources if not already downloaded\n",
        "        import nltk\n",
        "        nltk.download('punkt')\n",
        "        nltk.download('stopwords')\n",
        "\n",
        "        # Initialize tokenizers and stop words\n",
        "        self.word_tokenizer = word_tokenize\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        # self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # Initialize if using subword tokenization\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"Perform text preprocessing (e.g., lowercasing, removing punctuation)\"\"\"\n",
        "        text = text.lower()\n",
        "        # Add more preprocessing steps as needed\n",
        "        return text\n",
        "\n",
        "    def word_tokenize_text(self, text):\n",
        "        \"\"\"Perform word-level tokenization\"\"\"\n",
        "        tokens = self.word_tokenizer(text)\n",
        "        tokens = [token for token in tokens if token not in self.stop_words]\n",
        "        return tokens\n",
        "\n",
        "    def subword_tokenize_text(self, text):\n",
        "        \"\"\"Perform subword tokenization using BERT tokenizer\"\"\"\n",
        "        tokens = self.bert_tokenizer.tokenize(text)\n",
        "        tokens = [token for token in tokens if token not in self.stop_words]\n",
        "        return tokens\n",
        "\n",
        "    def sentence_tokenize_text(self, text):\n",
        "        \"\"\"Perform sentence-level tokenization\"\"\"\n",
        "        return sent_tokenize(text)\n",
        "\n",
        "    def process_chunk(self, texts):\n",
        "        \"\"\"Process a chunk of texts\"\"\"\n",
        "        results = []\n",
        "        for text in texts:\n",
        "            preprocessed_text = self.preprocess_text(text)\n",
        "\n",
        "            if self.tokenization_method == 'word':\n",
        "                tokens = self.word_tokenize_text(preprocessed_text)\n",
        "            elif self.tokenization_method == 'subword':\n",
        "                tokens = self.subword_tokenize_text(preprocessed_text)\n",
        "            elif self.tokenization_method == 'sentence':\n",
        "                tokens = self.sentence_tokenize_text(preprocessed_text)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown tokenization method: {self.tokenization_method}\")\n",
        "\n",
        "            results.append(tokens)\n",
        "        return results\n",
        "\n",
        "    def tokenize_dataset(self, texts):\n",
        "        \"\"\"Tokenize the entire dataset using multiprocessing\"\"\"\n",
        "        # Split texts into chunks for parallel processing\n",
        "        chunk_size = len(texts) // self.num_processes\n",
        "        chunks = [texts[i:i + chunk_size] for i in range(0, len(texts), chunk_size)]\n",
        "\n",
        "        # Process chunks in parallel\n",
        "        with mp.Pool(self.num_processes) as pool:\n",
        "            results = list(tqdm(pool.imap(self.process_chunk, chunks),\n",
        "                              total=len(chunks),\n",
        "                              desc=\"Tokenizing texts\"))\n",
        "\n",
        "        # Flatten results\n",
        "        return [token for chunk_result in results for token in chunk_result]\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Load the IMDB dataset\n",
        "    #df = pd.read_csv('/content/IMDB Dataset.csv.zip')\n",
        "    #The above line is causing the error, it should be replaced with:\n",
        "    df = pd.read_csv('IMDB Dataset.csv') # Changed the file path\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = IMDBTokenizer(tokenization_method='word', num_processes=4)\n",
        "\n",
        "    # Tokenize the"
      ],
      "metadata": {
        "id": "cQIRNncvWpEg"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#stemming to the dataset"
      ],
      "metadata": {
        "id": "GdtjvjBoXAV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#APPLICATION OF SNOWBALL STEMMER\n",
        "def preprocess_reviews(df, text_column):\n",
        "\n",
        "    # Download required NLTK data\n",
        "    try:\n",
        "        nltk.download('punkt')\n",
        "        nltk.download('stopwords')\n",
        "    except:\n",
        "        print(\"NLTK data already downloaded\")\n",
        "\n",
        "    # Initialize the Snowball stemmer (English by default)\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "kVmxBTCRDW9V"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying porter stemmer"
      ],
      "metadata": {
        "id": "o2BVmF6uYQzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer"
      ],
      "metadata": {
        "id": "SDxKSrTuYAjY"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps=PorterStemmer()\n",
        "def stem_words(text):\n",
        "  return \" \".join([ps.stem(word) for word in text.split()])\n",
        "  print(text)"
      ],
      "metadata": {
        "id": "sml7zbn3Yw0y"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lemmatization"
      ],
      "metadata": {
        "id": "mazp3aaYbpF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#without pos tagging\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "def simple_lemmatization(df, text_column):\n",
        "\n",
        "    try:\n",
        "        nltk.download('wordnet')\n",
        "    except:\n",
        "        print(\"NLTK data already downloaded\")\n",
        "\n",
        "    # Initialize lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def lemmatize_text(text):\n",
        "        \"\"\"\n",
        "        Lemmatize text without POS tagging\n",
        "        \"\"\"\n",
        "        # Split text into words\n",
        "        words = text.split()\n",
        "\n",
        "        # Apply basic lemmatization to each word\n",
        "        lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "        # Join words back into text\n",
        "        return ' '.join(lemmatized_words)\n",
        "\n",
        "    # Create a copy of the dataframe\n",
        "    lemmatized_df = df.copy()\n",
        "\n",
        "    # Apply lemmatization to the text column\n",
        "    lemmatized_df['lemmatized_text'] = lemmatized_df[text_column].apply(lemmatize_text)\n",
        "\n",
        "    return lemmatized_df\n"
      ],
      "metadata": {
        "id": "Iv0ZX_MvZB0z"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LHvnOp4idWA7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}