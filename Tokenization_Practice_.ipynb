{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7mUJ5VLr9bR",
        "outputId": "75226c51-af81-4a82-c324-5329f1103c91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=\"\"\"welcome to mansi's first nltk notebook\"\"\""
      ],
      "metadata": {
        "id": "_9WSRT-EsENG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnp5SrdSsWXX",
        "outputId": "afb06fcd-fb38-4b96-fb2c-0fa1401cc346"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "welcome to mansi's first nltk notebook\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "#tokenization\n",
        "#sentence-paragraph\n",
        "from nltk.tokenize import sent_tokenize\n",
        "sentences=sent_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fR6CFMyctFgO",
        "outputId": "8fd449a5-49da-4f64-d9a5-08c9117ee7a3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the 'punkt' resource if you haven't already\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Import the sent_tokenize function\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "corpus=\"\"\"welcome to mansi's first nltk notebook\"\"\"\n",
        "\n",
        "# Now you can use sent_tokenize\n",
        "documents=sent_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab5S72irKl5l",
        "outputId": "9fe097a9-6c07-49e1-87a8-6755854774c5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents=sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "I-aapCiXKOPL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6p5aoErKWO4",
        "outputId": "efa11f9b-1aba-4a12-fe11-a64eec3bb6f2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7G40ZPg0KrOE",
        "outputId": "6cc98570-5d7d-4290-dcaf-96001e175e69"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "welcome to mansi's first nltk notebook\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenization>>sentence-words\n",
        "#tokenization>>>paragraph into words\n",
        "#sentence--words\n",
        "from nltk.tokenize import word_tokenize\n",
        "words=word_tokenize(corpus)\n"
      ],
      "metadata": {
        "id": "Cb-Vpq93Kv54"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words=word_tokenize(corpus)#each and every word has its importance and it needs tokenization for efficiency"
      ],
      "metadata": {
        "id": "cNnHk6SEUKva"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "  print(word_tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huelKK64Ui0W",
        "outputId": "8666b5fb-e6c7-4393-c477-72d9fd377e3a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['welcome', 'to', 'mansi', \"'s\", 'first', 'nltk', 'notebook']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we have seen how to convert sentence into words\n",
        "#punkt tokenize\n",
        "#tree bank word tokenizer\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer=TreebankWordTokenizer()"
      ],
      "metadata": {
        "id": "gdH45zjHUsGO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function tokenizer = tokenize"
      ],
      "metadata": {
        "id": "ePCQDxSjVCwZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize(corpus)\n",
        "#function tokenizer = tokenize\n",
        "#with tree bank fullstop is not treated as separate word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05mr2CX-VKr5",
        "outputId": "9f054f4a-9496-4ba2-c9d8-436b579c46c5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['welcome', 'to', 'mansi', \"'s\", 'first', 'nltk', 'notebook']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "conversion in sentences and words was already done now we will jump to stemming"
      ],
      "metadata": {
        "id": "iX2yYVGFZQwn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming is the process of reducing word to its word stem that affixes to suffixes or prefixes or to the words of root known as lemma. stemming is imp for NLU and NLP"
      ],
      "metadata": {
        "id": "tXXn3qNQZc4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#classification problem\n",
        "#whether the comments in the product review are positive or negative\n",
        "#reviews--text data\n",
        "#The words like Eat , Eaten etc\n",
        "#each and every word represents a vector\n",
        "#lemmatization\n",
        "#stemming using Nltk\n",
        "words=[\"eating\",\"eats\",\"eaten\",\"programming\",\"programs\",\"program\",\"history\",\"histories\"]"
      ],
      "metadata": {
        "id": "Yj0T7geVZZrw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#porter stemmer"
      ],
      "metadata": {
        "id": "mvjGLlgWcZTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#porter stemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "stemming = PorterStemmer()\n",
        "for word in words:\n",
        "    print(word + \"-->\" + stemming.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqhdP7FBcT9T",
        "outputId": "87b7a729-3c43-4013-e022-4e4c0892636e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating-->eat\n",
            "eats-->eat\n",
            "eaten-->eaten\n",
            "programming-->program\n",
            "programs-->program\n",
            "program-->program\n",
            "history-->histori\n",
            "histories-->histori\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "for the word history we are getting histori that is the major issue with stemming that the word doesn't get replicated completely and sometimes there are erroroneous words might occur"
      ],
      "metadata": {
        "id": "j6zv-jJdfpaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem('congratulations')"
      ],
      "metadata": {
        "id": "MZ_kV3vSfozz",
        "outputId": "36519ebc-2d52-4c37-a309-ab553cfd33c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'congratul'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem('sitting')#stemming works for many words but it does bring up some errors"
      ],
      "metadata": {
        "id": "qv8bMY5DrveL",
        "outputId": "dc937164-8086-4b30-c0fb-59fd1e7e4222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sit'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we go for stemming in case of spam classification\n"
      ],
      "metadata": {
        "id": "EtMJn3ofr0uH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RegExpstemmer class"
      ],
      "metadata": {
        "id": "PHCjx1i3sHuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "nltk has RegExp stemmer class with the help of which we can easily implement Regular expression stemmer algorithms.It basically takes a single regular expression stemmer algorithms and remove any prefix or suffix that matches the expression"
      ],
      "metadata": {
        "id": "0MaUkXiJsP4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer # Change to 'RegexpStemmer' with a capital 'R'\n",
        "\n",
        "# Now you can create an instance:\n",
        "stemming = RegexpStemmer('ing$|s$|e$|able$', min=4)#ing $"
      ],
      "metadata": {
        "id": "2OUirqRKuPL6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer=RegexpStemmer"
      ],
      "metadata": {
        "id": "udfjohSLsMoq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A stemmer that uses regular expressions to understand morphological affixes"
      ],
      "metadata": {
        "id": "P49a1iQBvUGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "\n",
        "# Create an instance of RegexpStemmer\n",
        "reg_stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "\n",
        "# Now you can call the stem method on the instance\n",
        "stemmed_word = reg_stemmer.stem('eating')\n",
        "print(stemmed_word)"
      ],
      "metadata": {
        "id": "v_vys4EJv3ph",
        "outputId": "5f87e3cb-63a9-43fc-8306-4a09f70f71fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer.stem('ingeating')# coz  dollar after ing is put"
      ],
      "metadata": {
        "id": "R-tIOWZAvwh2",
        "outputId": "879fb29e-ae2f-4c7d-deac-a75b696914bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ingeat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regexstemmer class with which we can easily implement regular expressions stemmer algorithms. It basically takes a single regular expression and removes any prefix or suffix that matches the expression"
      ],
      "metadata": {
        "id": "gAfO0pYRylKn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#snowball stemmer"
      ],
      "metadata": {
        "id": "Fo4vBDDUzHf5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A stemming technique better performs better than porter stemmer"
      ],
      "metadata": {
        "id": "S_F04FeszJzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "d6FnSU6gzJN0"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Create an instance of the SnowballStemmer class for English\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "# Now you can use the stemmer object to stem words\n",
        "stemmed_word = stemmer.stem('running')\n",
        "print(stemmed_word)  # Output: run"
      ],
      "metadata": {
        "id": "oEp8bfJHzpUt",
        "outputId": "ea1d098c-ddac-4831-9c74-08c5f6f80825",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Create an instance of the SnowballStemmer class for English\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "# Assuming 'words' is a list of words you want to stem\n",
        "words = ['eating', 'histories', 'running']  # Replace with your actual list of words\n",
        "\n",
        "# Iterate through the words and stem them\n",
        "for word in words:\n",
        "  print(word + \"--->\" + stemmer.stem(word))"
      ],
      "metadata": {
        "id": "0sUdcDaF0R_Y",
        "outputId": "60e1abd6-9ddb-4113-cc43-416703b52db2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating--->eat\n",
            "histories--->histori\n",
            "running--->run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem(\"fairly\")"
      ],
      "metadata": {
        "id": "5DOZdh3H0UjK",
        "outputId": "32692ed4-d0e1-4202-a2cc-e0b13e013583",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fairly'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Create an instance of the SnowballStemmer class for English\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "# Stem the word 'fairly'\n",
        "stemmed_word = stemmer.stem('fairly')  # Use the correct variable name 'stemmer'\n",
        "print(stemmed_word)"
      ],
      "metadata": {
        "id": "c0k-Cbzz2nbq",
        "outputId": "5d900401-e1dd-4761-c2ad-7267fd01d253",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Initialize the stemmer (English is used here, but you can choose other languages)\n",
        "snowball_stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# Now you can use the stemmer\n",
        "stemmed_word = snowball_stemmer.stem('goes')\n",
        "print(stemmed_word)"
      ],
      "metadata": {
        "id": "nVg7yypE5m9I",
        "outputId": "645bb263-3898-4047-81ed-252f15158a85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "goe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For some of the word the form of words the input gets erroneous\n",
        "lemmatization solves the problem it solves the issues of these errors"
      ],
      "metadata": {
        "id": "vxIq5N8P6wJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#stemming is process of reducing the word to its stem\n",
        "#for some words it work using technique like porter stemmer\n"
      ],
      "metadata": {
        "id": "kh8l1dv57Qog"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lemmatization\n",
        "lemmatization technique is like stemming . The output we get after lemmatization is called \"lemma\", which is a root word rather than root stem, the output of stemming . After Lemmatization we will be getting a valid word which means the same thing\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UH1BgPW-8CeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK provides wordNetLemmatizer class which is a thin wrapper around the wordnet corpus. This class uses the morphy()function to the wordnet corpus reader class to find a lemma.Let us understand it with an example"
      ],
      "metadata": {
        "id": "MKXt7kmS8sj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "G79mEcbL9d4n"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Lemmatizer=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "BpBAhfriCUDD"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download the 'wordnet' dataset\n",
        "nltk.download('wordnet')\n",
        "\n",
        "Lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Use the correctly named variable 'Lemmatizer' instead of 'lemmatizer'\n",
        "Lemmatizer.lemmatize(\"going\")  # It has 2 properties: one is word and another is postag\n",
        "\n",
        "# The variable 'lemmatizer' was not defined, it should be 'Lemmatizer'\n",
        "Lemmatizer.lemmatize(\"going\",pos='v') #it has 2 properties one is word and another is postag\n",
        "#for adverb=r\n",
        "#for adjective=a\n",
        "#for verb=v"
      ],
      "metadata": {
        "id": "wx7UNHHtDL-r",
        "outputId": "72d14f01-3da0-4e56-89df-289fa37bfe18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pos\n",
        "noun-n\n",
        "adverb-r\n",
        "adjective-a"
      ],
      "metadata": {
        "id": "HB9xLcDqDAmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words=['eating','eats','eaten','programming','programs','program','history','histories']"
      ],
      "metadata": {
        "id": "GfSSacLeDjRo"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word + \"-->\" + Lemmatizer.lemmatize(word,pos='v'))"
      ],
      "metadata": {
        "id": "F3oSJtpRDmc0",
        "outputId": "d64b0619-593f-4c54-d228-bb6ea0d0d5c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating-->eat\n",
            "eats-->eat\n",
            "eaten-->eat\n",
            "programming-->program\n",
            "programs-->program\n",
            "program-->program\n",
            "history-->history\n",
            "histories-->histories\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "for noun and pronouns the lemmatization doesn't occur"
      ],
      "metadata": {
        "id": "i2qNAxYSDvAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatizer takes more time \\\n",
        "\n",
        "\n",
        "*   chat bots\n",
        "*   Ques and Ans\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bS_QExBcFhYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stopwords\n",
        "conversion of words into vectors\n",
        "we give input data in form of numerical and input values and perform mathematical implications"
      ],
      "metadata": {
        "id": "smJ7mbz5GD1s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why is imp to remove stop words?\n",
        "Removing stop words is an important step in many Natural Language Processing (NLP) tasks. Here's an explanation of why we often remove stop words:\n",
        "\n",
        "1. Noise reduction: Stop words are very common words (like \"the\", \"is\", \"at\", \"which\", \"on\") that usually don't carry much meaning in text analysis. Removing them helps reduce noise in the data.\n",
        "\n",
        "2. Improved efficiency: By eliminating these frequent but low-value words, we can reduce the size of our dataset, which can lead to faster processing times and lower memory usage.\n",
        "\n",
        "3. Focus on important content: Removing stop words allows algorithms to focus on the more important words that actually convey the main ideas or sentiment of the text.\n",
        "\n",
        "4. Better feature extraction: In many NLP tasks, we convert text into numerical features. Stop words can skew these features, so removing them can lead to more meaningful feature representations.\n",
        "\n",
        "5. Improved performance in certain tasks: For tasks like document classification or clustering, removing stop words can sometimes improve performance by reducing the impact of non-informative words.\n",
        "\n",
        "However, it's important to note that stop word removal isn't always beneficial. In some cases, such as sentiment analysis or certain types of text generation, stop words might carry important contextual information. The decision to remove stop words should be made based on the specific NLP task and the nature of your data.\n",
        "\n",
        "Would you like me to provide an example of how stop word removal works in practice?"
      ],
      "metadata": {
        "id": "25jpVYTCGQgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph=\"\"\"Dear Young Friends, I am delighted to be among you today. You are the future of our great nation, and in your hands lies the power to shape India's destiny. Dreams are not what you see in sleep. Dreams are those that don't let you sleep. They are the fuel that drives innovation and progress. I want you to dream. Dream of a strong, prosperous, and self-reliant India. Because it is only when we dream big that we can achieve great things. A dream is not enough. You need a vision to transform that dream into reality. Ask yourself: What kind of India do you want to see in the next decade? In the next century? Envision an India free from poverty, illiteracy, and corruption. An India that stands tall among the community of nations. Vision without action is merely a dream. Action without vision just passes the time. Vision with action can change the world. Set clear goals for yourself and work tirelessly to achieve them. Remember, success is when your signature becomes an autograph. The path to success is never easy. You will face obstacles, criticism, and failures. But remember, failure is the stepping stone to success. In my life, I have faced many setbacks, but each one taught me valuable lessons. Learn from your failures, but never let them define you. Knowledge is the greatest equalizer in the world. In this rapidly changing world, continuous learning is not just an option; it's a necessity. Read voraciously, question constantly, and always be curious. The more you learn, the more you can contribute to society. As you strive for success, never compromise on your integrity. Your character is your greatest asset. Be honest, be compassionate, and always stand up for what is right. A nation's greatness is measured not just by its GDP, but by the character of its citizens. True fulfillment comes not from personal achievements alone, but from contributing to the greater good. Use your knowledge, skills, and resources to uplift those less fortunate. A developed India is one where every citizen has the opportunity to realize their potential. My dear young friends, you are the pillars upon which the future of our nation rests. You have the power to transform India into a developed nation. Dream big, work hard, and never give up. Remember, you are the creators of the India of tomorrow. Thank you. May God bless you.\"\"\""
      ],
      "metadata": {
        "id": "73fciEZND4B6"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "DbsGJpTzIWOg",
        "outputId": "8839c395-c392-4952-f1b8-274958fa850c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords') # This downloads the stopwords resource\n",
        "from nltk.corpus import stopwords # This imports the stopwords object\n",
        "\n",
        "stopwords.words('english')"
      ],
      "metadata": {
        "id": "cLDrUmWOJiuw",
        "outputId": "a1fdae69-9b78-42f1-b1aa-5ea476a6250f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "Y1P12KMeH-6d"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "RKswDNBIIJkJ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('french')"
      ],
      "metadata": {
        "id": "THLelilpIPvh",
        "outputId": "a0bddd46-49a6-4d50-ed65-11784c6c5236",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['au',\n",
              " 'aux',\n",
              " 'avec',\n",
              " 'ce',\n",
              " 'ces',\n",
              " 'dans',\n",
              " 'de',\n",
              " 'des',\n",
              " 'du',\n",
              " 'elle',\n",
              " 'en',\n",
              " 'et',\n",
              " 'eux',\n",
              " 'il',\n",
              " 'ils',\n",
              " 'je',\n",
              " 'la',\n",
              " 'le',\n",
              " 'les',\n",
              " 'leur',\n",
              " 'lui',\n",
              " 'ma',\n",
              " 'mais',\n",
              " 'me',\n",
              " 'même',\n",
              " 'mes',\n",
              " 'moi',\n",
              " 'mon',\n",
              " 'ne',\n",
              " 'nos',\n",
              " 'notre',\n",
              " 'nous',\n",
              " 'on',\n",
              " 'ou',\n",
              " 'par',\n",
              " 'pas',\n",
              " 'pour',\n",
              " 'qu',\n",
              " 'que',\n",
              " 'qui',\n",
              " 'sa',\n",
              " 'se',\n",
              " 'ses',\n",
              " 'son',\n",
              " 'sur',\n",
              " 'ta',\n",
              " 'te',\n",
              " 'tes',\n",
              " 'toi',\n",
              " 'ton',\n",
              " 'tu',\n",
              " 'un',\n",
              " 'une',\n",
              " 'vos',\n",
              " 'votre',\n",
              " 'vous',\n",
              " 'c',\n",
              " 'd',\n",
              " 'j',\n",
              " 'l',\n",
              " 'à',\n",
              " 'm',\n",
              " 'n',\n",
              " 's',\n",
              " 't',\n",
              " 'y',\n",
              " 'été',\n",
              " 'étée',\n",
              " 'étées',\n",
              " 'étés',\n",
              " 'étant',\n",
              " 'étante',\n",
              " 'étants',\n",
              " 'étantes',\n",
              " 'suis',\n",
              " 'es',\n",
              " 'est',\n",
              " 'sommes',\n",
              " 'êtes',\n",
              " 'sont',\n",
              " 'serai',\n",
              " 'seras',\n",
              " 'sera',\n",
              " 'serons',\n",
              " 'serez',\n",
              " 'seront',\n",
              " 'serais',\n",
              " 'serait',\n",
              " 'serions',\n",
              " 'seriez',\n",
              " 'seraient',\n",
              " 'étais',\n",
              " 'était',\n",
              " 'étions',\n",
              " 'étiez',\n",
              " 'étaient',\n",
              " 'fus',\n",
              " 'fut',\n",
              " 'fûmes',\n",
              " 'fûtes',\n",
              " 'furent',\n",
              " 'sois',\n",
              " 'soit',\n",
              " 'soyons',\n",
              " 'soyez',\n",
              " 'soient',\n",
              " 'fusse',\n",
              " 'fusses',\n",
              " 'fût',\n",
              " 'fussions',\n",
              " 'fussiez',\n",
              " 'fussent',\n",
              " 'ayant',\n",
              " 'ayante',\n",
              " 'ayantes',\n",
              " 'ayants',\n",
              " 'eu',\n",
              " 'eue',\n",
              " 'eues',\n",
              " 'eus',\n",
              " 'ai',\n",
              " 'as',\n",
              " 'avons',\n",
              " 'avez',\n",
              " 'ont',\n",
              " 'aurai',\n",
              " 'auras',\n",
              " 'aura',\n",
              " 'aurons',\n",
              " 'aurez',\n",
              " 'auront',\n",
              " 'aurais',\n",
              " 'aurait',\n",
              " 'aurions',\n",
              " 'auriez',\n",
              " 'auraient',\n",
              " 'avais',\n",
              " 'avait',\n",
              " 'avions',\n",
              " 'aviez',\n",
              " 'avaient',\n",
              " 'eut',\n",
              " 'eûmes',\n",
              " 'eûtes',\n",
              " 'eurent',\n",
              " 'aie',\n",
              " 'aies',\n",
              " 'ait',\n",
              " 'ayons',\n",
              " 'ayez',\n",
              " 'aient',\n",
              " 'eusse',\n",
              " 'eusses',\n",
              " 'eût',\n",
              " 'eussions',\n",
              " 'eussiez',\n",
              " 'eussent']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "zga1XupXKUjn"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer=PorterStemmer()"
      ],
      "metadata": {
        "id": "OOU0oY-hKdA1"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "PGIJdlWnLnqs"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(sentences)"
      ],
      "metadata": {
        "id": "Bv6uBR4CL0RD",
        "outputId": "a212cd78-e4a9-43be-f10d-306824d5bd50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Apply stopwords then filter and then stemming"
      ],
      "metadata": {
        "id": "cfyNsVraMAOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph=\"\"\"Dear Young Friends, I am delighted to be among you today. You are the future of our great nation, and in your hands lies the power to shape India's destiny. Dreams are not what you see in sleep. Dreams are those that don't let you sleep. They are the fuel that drives innovation and progress. I want you to dream. Dream of a strong, prosperous, and self-reliant India. Because it is only when we dream big that we can achieve great things. A dream is not enough. You need a vision to transform that dream into reality. Ask yourself: What kind of India do you want to see in the next decade? In the next century? Envision an India free from poverty, illiteracy, and corruption. An India that stands tall among the community of nations. Vision without action is merely a dream. Action without vision just passes the time. Vision with action can change the world. Set clear goals for yourself and work tirelessly to achieve them. Remember, success is when your signature becomes an autograph. The path to success is never easy. You will face obstacles, criticism, and failures. But remember, failure is the stepping stone to success. In my life, I have faced many setbacks, but each one taught me valuable lessons. Learn from your failures, but never let them define you. Knowledge is the greatest equalizer in the world. In this rapidly changing world, continuous learning is not just an option; it's a necessity. Read voraciously, question constantly, and always be curious. The more you learn, the more you can contribute to society. As you strive for success, never compromise on your integrity. Your character is your greatest asset. Be honest, be compassionate, and always stand up for what is right. A nation's greatness is measured not just by its GDP, but by the character of its citizens. True fulfillment comes not from personal achievements alone, but from contributing to the greater good. Use your knowledge, skills, and resources to uplift those less fortunate. A developed India is one where every citizen has the opportunity to realize their potential. My dear young friends, you are the pillars upon which the future of our nation rests. You have the power to transform India into a developed nation. Dream big, work hard, and never give up. Remember, you are the creators of the India of tomorrow. Thank you. May God bless you.\"\"\""
      ],
      "metadata": {
        "id": "sWMJTiQ_SrDS"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences)):\n",
        "  nltk.word_tokenize(sentences[i])\n",
        "  [stemmer.stem(word) for word in word if not word in set(stopwords.words('english'))]# we get a specific word that is not there in the stopowords only stemming will be apply for that specific words\n",
        "  sentences[i]=' '.join(words)#converting all the words into sentences\n",
        "  #converting the list of words into sentences\n"
      ],
      "metadata": {
        "id": "8sT1jj2dL2Ae"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "id": "0DRhzY2NOIlV",
        "outputId": "d5d4465d-aaff-47e2-831b-4b92b993914a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories',\n",
              " 'eating eats eaten programming programs program history histories']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences)):\n",
        "  words=nltk.word_tokenize(sentences[i])\n",
        "  words = [Lemmatizer.lemmatize(word, pos='v') for word in words if not word in set(stopwords.words('english'))] # we get a specific word that is not there in the stopowords only stemming will be apply for that specific words\n",
        "  sentences[i]=' '.join(words)#converting all the words into sentences\n",
        "  #converting the list of words into sentences"
      ],
      "metadata": {
        "id": "N9tUukmvJr7w"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "id": "GgObuTrqKRyH",
        "outputId": "2a6f751e-3bda-4183-e3f8-b243bc40e76d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories',\n",
              " 'eat eat eat program program program history histories']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowball_stemmer = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "IkI4YeR3KS7K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}